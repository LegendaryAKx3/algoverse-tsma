{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcaZJr2Q3DgpaX93pVbmb0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LegendaryAKx3/algoverse-tsma/blob/main/KuhnPokerRiskEstimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup"
      ],
      "metadata": {
        "id": "Olg2K-_jwSZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_spiel"
      ],
      "metadata": {
        "id": "6_9ss6vIDdo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspiel\n",
        "from open_spiel.python.algorithms import outcome_sampling_mccfr\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "AJgNHbsbEh-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = pyspiel.load_game(\"kuhn_poker\")\n",
        "\n",
        "cfr_solver = outcome_sampling_mccfr.OutcomeSamplingSolver(game)"
      ],
      "metadata": {
        "id": "l3MQVhBDEijX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Training"
      ],
      "metadata": {
        "id": "Gp1aXB2lqozs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run MCCFR for 10,000 iterations to converge to a Nash equilibrium\n",
        "print(\"Running MCCFR for 10,000 iterations...\")\n",
        "iterations = 10000\n",
        "for i in range(iterations):\n",
        "    cfr_solver.iteration()\n",
        "    if (i + 1) % 2000 == 0:\n",
        "        print(f\"  ...completed {i + 1} iterations\")\n",
        "print(\"Training complete.\")\n",
        "\n",
        "policy = cfr_solver.average_policy()"
      ],
      "metadata": {
        "id": "gxhsjnmbFGfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Print Policy Function"
      ],
      "metadata": {
        "id": "0xRr2uyHwaMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_policy(policy, game):\n",
        "    \"\"\"Traverses the game tree to print the policy for each information set.\"\"\"\n",
        "    print(\"\\n## Nash Equilibrium Strategies\")\n",
        "\n",
        "    # Dictionary to store one representative state per information set\n",
        "    info_set_states = {}\n",
        "\n",
        "    # Recursive function to find a representative state for each info set\n",
        "    def traverse(state):\n",
        "        if state.is_terminal():\n",
        "            return\n",
        "        if state.is_chance_node():\n",
        "            for action, _ in state.chance_outcomes():\n",
        "                traverse(state.child(action))\n",
        "            return\n",
        "\n",
        "        info_set = state.information_state_string()\n",
        "        player = state.current_player()\n",
        "\n",
        "        if (info_set, player) not in info_set_states:\n",
        "            info_set_states[(info_set, player)] = state.clone()\n",
        "\n",
        "        for action in state.legal_actions():\n",
        "            traverse(state.child(action))\n",
        "\n",
        "    traverse(game.new_initial_state())\n",
        "\n",
        "    # Print action probabilities for each information set, sorted for consistency\n",
        "    for info_set, player in sorted(info_set_states.keys()):\n",
        "        state = info_set_states[(info_set, player)]\n",
        "        action_probs = policy.action_probabilities(state)\n",
        "\n",
        "        action_names = {a: game.action_to_string(player, a) for a in action_probs.keys()}\n",
        "\n",
        "        print(f\"\\n**Player {player} | Info Set: `{info_set}`**\")\n",
        "        for action, prob in action_probs.items():\n",
        "            print(f\"  - Action `{action_names[action]}`: {prob:.3f}\")\n"
      ],
      "metadata": {
        "id": "aGVRhdl6FMWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Risk Calculation Function"
      ],
      "metadata": {
        "id": "WjgqrWk7weAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_risk(info_set_str, action_str, policy, game):\n",
        "    \"\"\"\n",
        "    Calculates the expected payoff and loss probability for a given action in a specific info set.\n",
        "    This version correctly reconstructs game states instead of using deprecated methods.\n",
        "    \"\"\"\n",
        "    # Deconstruct the info set string to understand the game state\n",
        "    player_card_char = info_set_str[0]\n",
        "    history_str = info_set_str[1:]\n",
        "\n",
        "    player = len(history_str) % 2\n",
        "\n",
        "    card_map = {'J': 0, 'Q': 1, 'K': 2}\n",
        "    player_card = card_map[player_card_char]\n",
        "\n",
        "    opponent_cards = [c for c in card_map.values() if c != player_card]\n",
        "\n",
        "    total_expected_payoff = 0.0\n",
        "    total_loss_prob = 0.0\n",
        "\n",
        "    belief = 1.0 / len(opponent_cards)\n",
        "\n",
        "    # Iterate through each \"possible world\" (each possible opponent hand)\n",
        "    for opp_card in opponent_cards:\n",
        "\n",
        "        # Reconstruct the specific game state from this world\n",
        "        if player == 0:\n",
        "            deal = [player_card, opp_card]\n",
        "        else:\n",
        "            deal = [opp_card, player_card]\n",
        "\n",
        "        # Find the initial state corresponding to this specific deal\n",
        "        state = None\n",
        "        state = game.new_initial_state()\n",
        "        state.apply_action(deal[0])\n",
        "        state.apply_action(deal[1])\n",
        "\n",
        "        action_map = {'p': 0, 'b': 1}\n",
        "        for act_char in history_str:\n",
        "            state.apply_action(action_map[act_char])\n",
        "\n",
        "        action_map = {'Pass': 0, 'Bet': 1, 'p': 0, 'b': 1}\n",
        "        action_to_eval = action_map[action_str]\n",
        "\n",
        "        state_after_action = state.clone()\n",
        "        state_after_action.apply_action(action_to_eval)\n",
        "\n",
        "        world_payoff = 0.0\n",
        "        world_loss_prob = 0.0\n",
        "\n",
        "        if state_after_action.is_terminal():\n",
        "            payoff = state_after_action.returns()[player]\n",
        "            world_payoff = payoff\n",
        "            if payoff < 0:\n",
        "                world_loss_prob = 1.0\n",
        "        else:\n",
        "            opp_policy = policy.action_probabilities(state_after_action)\n",
        "            for opp_action, opp_prob in opp_policy.items():\n",
        "                terminal_state = state_after_action.clone()\n",
        "                terminal_state.apply_action(opp_action)\n",
        "                payoff = terminal_state.returns()[player]\n",
        "\n",
        "                world_payoff += opp_prob * payoff\n",
        "                if payoff < 0:\n",
        "                    world_loss_prob += opp_prob\n",
        "\n",
        "        total_expected_payoff += belief * world_payoff\n",
        "        total_loss_prob += belief * world_loss_prob\n",
        "\n",
        "    return total_expected_payoff, total_loss_prob"
      ],
      "metadata": {
        "id": "cJTbSwIgCdN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Execution"
      ],
      "metadata": {
        "id": "CEXDFqRTwmu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_policy(policy, game)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"## Risk Calculation Example\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Format: [Card][Action History]\n",
        "# First character: J(Jack), Q(Queen), or K(King) - the player's card\n",
        "# Remaining characters: p(pass) or b(bet) - sequence of actions taken, Player 0, then Player 1, then Player 0, etc\n",
        "# Example: \"Kp\" = Player has King, opponent passed\n",
        "info_set = \"Jp\"\n",
        "action = \"Bet\" #Bet or Pass\n",
        "\n",
        "expected_payoff, loss_prob = calculate_risk(info_set, action, policy, game)\n",
        "\n",
        "print(f\"\\nCalculating risk for Player 1 with Info Set `{info_set}` and Action `{action}`...\")\n",
        "print(f\"\\n  - **Expected Payoff:** ${expected_payoff:.3f}\")\n",
        "print(f\"  - **Probability of Loss:** {loss_prob:.3f} ({loss_prob:.1%})\")"
      ],
      "metadata": {
        "id": "3a3RARRpwmI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}